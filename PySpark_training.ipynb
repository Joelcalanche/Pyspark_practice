{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surrounded-carbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "Collecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805913 sha256=3366e5711d0c155fc7a92323d9490ae42bcfed36304196e790f2c30ff599d72b\n",
      "  Stored in directory: c:\\users\\calanche\\appdata\\local\\pip\\cache\\wheels\\2f\\f8\\95\\2ad14a4614b4a9f645ee928fbbd057b1b254c67adb494c9a58\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "economic-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "quick-portal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "type(pd.read_csv(\"test_1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developing-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "green-webcam",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "certain-small",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://JOEL.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x16674054e20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "second-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tender-liquid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "civic-knife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|  _c0|_c1|\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "| joel| 25|\n",
      "|pedro| 23|\n",
      "|Sunny| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "removable-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_2 = spark.read.option('header','true').csv(\"test_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "subsequent-township",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "| joel| 25|\n",
      "|pedro| 23|\n",
      "|Sunny| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "waiting-interaction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collective-florist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Name='joel', Age='25')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "colored-filename",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-festival",
   "metadata": {},
   "source": [
    "In this Video We wil Cover\n",
    "\n",
    "* PySpark DataFrame\n",
    "\n",
    "* Reading the DataSet\n",
    "\n",
    "* Checking the Datatypes of the column(Schema)\n",
    "\n",
    "* Selecting Columns And indexing\n",
    "\n",
    "* Adding Columns\n",
    "\n",
    "* Dropping columns\n",
    "\n",
    "* Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "balanced-killing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://JOEL.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x16674054e20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vanilla-resistance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| Name|age|experience|\n",
      "+-----+---+----------+\n",
      "|krish| 31|        10|\n",
      "| hola| 30|         8|\n",
      "|Sunny| 29|         4|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## read the dataset\n",
    "df_pyspark_3 = spark.read.option('header','true').csv(\"test_2.csv\", inferSchema=True)\n",
    "\n",
    "df_pyspark_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "further-priest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Check the schema\n",
    "\n",
    "df_pyspark_3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "amended-click",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| Name|age|experience|\n",
      "+-----+---+----------+\n",
      "|krish| 31|        10|\n",
      "| hola| 30|         8|\n",
      "|Sunny| 29|         4|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv('test_2.csv', header=True, inferSchema=True)\n",
    "\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "convertible-trader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "universal-update",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'experience']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "electronic-diameter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='krish', age=31, experience=10),\n",
       " Row(Name='hola', age=30, experience=8),\n",
       " Row(Name='Sunny', age=29, experience=4)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "proof-heart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| Name|age|experience|\n",
      "+-----+---+----------+\n",
      "|krish| 31|        10|\n",
      "| hola| 30|         8|\n",
      "|Sunny| 29|         4|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "beginning-spyware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| Name|\n",
      "+-----+\n",
      "|krish|\n",
      "| hola|\n",
      "|Sunny|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adaptive-harvey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark.select(\"Name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efficient-discharge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| Name|Experience|\n",
      "+-----+----------+\n",
      "|krish|        10|\n",
      "| hola|         8|\n",
      "|Sunny|         4|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name', 'Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "phantom-sending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('age', 'int'), ('experience', 'int')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "australian-break",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+-----------------+\n",
      "|summary| Name| age|       experience|\n",
      "+-------+-----+----+-----------------+\n",
      "|  count|    3|   3|                3|\n",
      "|   mean| null|30.0|7.333333333333333|\n",
      "| stddev| null| 1.0|3.055050463303893|\n",
      "|    min|Sunny|  29|                4|\n",
      "|    max|krish|  31|               10|\n",
      "+-------+-----+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "incorporate-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding Columns in data frame\n",
    "\n",
    "df_pyspark=df_pyspark.withColumn('Experience After 2 year', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "polish-consultancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='krish', age=31, experience=10, Experience After 2 year=12),\n",
       " Row(Name='hola', age=30, experience=8, Experience After 2 year=10),\n",
       " Row(Name='Sunny', age=29, experience=4, Experience After 2 year=6)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fatal-presence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+-----------------------+\n",
      "| Name|age|experience|Experience After 2 year|\n",
      "+-----+---+----------+-----------------------+\n",
      "|krish| 31|        10|                     12|\n",
      "| hola| 30|         8|                     10|\n",
      "|Sunny| 29|         4|                      6|\n",
      "+-----+---+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "chief-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+\n",
      "| Name|age|experience|\n",
      "+-----+---+----------+\n",
      "|krish| 31|        10|\n",
      "| hola| 30|         8|\n",
      "|Sunny| 29|         4|\n",
      "+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Drop the columns\n",
    "\n",
    "df_pyspark = df_pyspark.drop('Experience after 2 year')\n",
    "\n",
    "\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "minus-employment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|New Name|age|experience|\n",
      "+--------+---+----------+\n",
      "|   krish| 31|        10|\n",
      "|    hola| 30|         8|\n",
      "|   Sunny| 29|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Rename the columns\n",
    "df_pyspark.withColumnRenamed('Name', 'New Name').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-recording",
   "metadata": {},
   "source": [
    "## Pyspark Handling Missing Values\n",
    "\n",
    "* Dropping Columns\n",
    "\n",
    "* Dropping Rows\n",
    "\n",
    "* Various Parameter in Dropping functionalities\n",
    "\n",
    "* Handling Missing values by Mean, Median, and Mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "junior-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.csv('test_3.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "established-representative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|  krish|  11|        10| 30000|\n",
      "| sudhan|  30|         8| 25000|\n",
      "|  sunny|  29|         4| 20000|\n",
      "|   Paul|  23|         3| 20000|\n",
      "| Harsha|  21|         1| 15000|\n",
      "|Shubham|  23|         2| 18000|\n",
      "| Mahesh|null|      null| 40000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  36|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "statewide-institution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##drop the columns\n",
    "\n",
    "df_spark.drop('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fitted-renaissance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|  krish|  11|        10| 30000|\n",
      "| sudhan|  30|         8| 25000|\n",
      "|  sunny|  29|         4| 20000|\n",
      "|   Paul|  23|         3| 20000|\n",
      "| Harsha|  21|         1| 15000|\n",
      "|Shubham|  23|         2| 18000|\n",
      "| Mahesh|null|      null| 40000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  36|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "center-abuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  krish| 11|        10| 30000|\n",
      "| sudhan| 30|         8| 25000|\n",
      "|  sunny| 29|         4| 20000|\n",
      "|   Paul| 23|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "amber-lingerie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|  krish|  11|        10| 30000|\n",
      "| sudhan|  30|         8| 25000|\n",
      "|  sunny|  29|         4| 20000|\n",
      "|   Paul|  23|         3| 20000|\n",
      "| Harsha|  21|         1| 15000|\n",
      "|Shubham|  23|         2| 18000|\n",
      "| Mahesh|null|      null| 40000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  36|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### any==how\n",
    "df_spark.na.drop(how=\"all\").show() # elimina filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "weighted-classroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|  krish|  11|        10| 30000|\n",
      "| sudhan|  30|         8| 25000|\n",
      "|  sunny|  29|         4| 20000|\n",
      "|   Paul|  23|         3| 20000|\n",
      "| Harsha|  21|         1| 15000|\n",
      "|Shubham|  23|         2| 18000|\n",
      "| Mahesh|null|      null| 40000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Threshold, cuantos no null considero para eliminar(si tengo menos valores no nulos elimino)\n",
    "df_spark.na.drop(how=\"any\", subset=[\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "offshore-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Subset only specific columms\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "sought-monster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| Age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|         krish|  11|        10| 30000|\n",
      "|        sudhan|  30|         8| 25000|\n",
      "|         sunny|  29|         4| 20000|\n",
      "|          Paul|  23|         3| 20000|\n",
      "|        Harsha|  21|         1| 15000|\n",
      "|       Shubham|  23|         2| 18000|\n",
      "|        Mahesh|null|      null| 40000|\n",
      "|Missing Values|  34|        10| 38000|\n",
      "|Missing Values|  36|      null|  null|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### filling the Missing value\n",
    "\n",
    "df_spark.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "foreign-coating",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \" Experience\" among (Name, Age, Experience, Salary)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-026b0e201688>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing Values'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' Experience'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\sample_project_1\\env\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mfill\u001b[1;34m(self, value, subset)\u001b[0m\n\u001b[0;32m   2787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2788\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2789\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2791\u001b[0m     \u001b[0mfill\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\sample_project_1\\env\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mfillna\u001b[1;34m(self, value, subset)\u001b[0m\n\u001b[0;32m   2104\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"subset should be a list or tuple of column names\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2106\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_replace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\sample_project_1\\env\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\sample_project_1\\env\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Cannot resolve column name \" Experience\" among (Name, Age, Experience, Salary)"
     ]
    }
   ],
   "source": [
    "df_spark.na.fill('Missing Values', ' Experience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "modern-generator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|  krish|  11|        10| 30000|\n",
      "| sudhan|  30|         8| 25000|\n",
      "|  sunny|  29|         4| 20000|\n",
      "|   Paul|  23|         3| 20000|\n",
      "| Harsha|  21|         1| 15000|\n",
      "|Shubham|  23|         2| 18000|\n",
      "| Mahesh|null|      null| 40000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  36|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "applicable-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"Age\", \"Experience\", 'Salary'],\n",
    "    outputCols=[\"{}_inputed\".format(c) for c in ['Age', 'Experience', 'Salary']]\n",
    ").setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "outside-blond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|   Name| Age|Experience|Salary|Age_inputed|Experience_inputed|Salary_inputed|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "|  krish|  11|        10| 30000|         11|                10|         30000|\n",
      "| sudhan|  30|         8| 25000|         30|                 8|         25000|\n",
      "|  sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|   Paul|  23|         3| 20000|         23|                 3|         20000|\n",
      "| Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "| Mahesh|null|      null| 40000|         25|                 5|         40000|\n",
      "|   null|  34|        10| 38000|         34|                10|         38000|\n",
      "|   null|  36|      null|  null|         36|                 5|         25750|\n",
      "+-------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "\n",
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-graphics",
   "metadata": {},
   "source": [
    "## Pyspark Dataframes\n",
    "\n",
    "* Filter Operation\n",
    "\n",
    "* & , |,  ==\n",
    "\n",
    "* ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "frozen-stack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|  krish|  11|        10| 30000|\n",
      "| sudhan|  30|         8| 25000|\n",
      "|  sunny|  29|         4| 20000|\n",
      "|   Paul|  23|         3| 20000|\n",
      "| Harsha|  21|         1| 15000|\n",
      "|Shubham|  23|         2| 18000|\n",
      "| Mahesh|null|      null| 40000|\n",
      "|   null|  34|        10| 38000|\n",
      "|   null|  36|      null|  null|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-transfer",
   "metadata": {},
   "source": [
    "### Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "better-mandate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  sunny| 29|         4| 20000|\n",
      "|   Paul| 23|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Salary of the people less thanor equal to 20000\n",
    "\n",
    "df_spark.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "neutral-accounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|age|\n",
      "+-------+---+\n",
      "|  sunny| 29|\n",
      "|   Paul| 23|\n",
      "| Harsha| 21|\n",
      "|Shubham| 23|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter(\"Salary<=20000\").select([\"Name\", 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "honey-collaboration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  sunny| 29|         4| 20000|\n",
      "|   Paul| 23|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter(df_spark['Salary']<=20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "following-basis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  sunny| 29|         4| 20000|\n",
      "|   Paul| 23|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter((df_spark['Salary']<=20000) & (df_spark['Salary']>=15000) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "environmental-oasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "| krish|  11|        10| 30000|\n",
      "|sudhan|  30|         8| 25000|\n",
      "|Mahesh|null|      null| 40000|\n",
      "|  null|  34|        10| 38000|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ~ significa negacion\n",
    "df_spark.filter(~(df_spark['Salary']<=20000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-lodging",
   "metadata": {},
   "source": [
    "## Pyspark GroupBy And Aggregate Fuctions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "subject-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.csv('test_4.csv', header=True, inferSchema=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "applied-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name|  Department|salary|\n",
      "+---------+------------+------+\n",
      "|    krish|Data Science| 10000|\n",
      "|    krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|sudhanshu|Data Science| 20000|\n",
      "|sudhanshu|         IOT| 10000|\n",
      "|sudhanshu|    Big Data|  5000|\n",
      "|sudhanshu|Data Science| 10000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "future-polish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "tough-personality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|    krish|      19000|\n",
      "|sudhanshu|      45000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Groupby\n",
    "## Grouepd to find the maximun salary\n",
    "df_spark.groupBy('Name').sum().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "exotic-exhibit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "|  Department|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      13000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Groupby Deparments which gives maximum salary\n",
    "\n",
    "df_spark.groupBy('Department').sum().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fiscal-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|  Department|      avg(salary)|\n",
      "+------------+-----------------+\n",
      "|         IOT|           7500.0|\n",
      "|    Big Data|4333.333333333333|\n",
      "|Data Science|          10750.0|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupBy('Department').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fatal-rhythm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|  Department|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    3|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupBy('Department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "deluxe-positive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      71000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "friendly-forward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|max(salary)|\n",
      "+---------+-----------+\n",
      "|    krish|      10000|\n",
      "|sudhanshu|      20000|\n",
      "|   Mahesh|       4000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "identical-differential",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|max(salary)|\n",
      "+---------+-----------+\n",
      "|    krish|      10000|\n",
      "|sudhanshu|      20000|\n",
      "|   Mahesh|       4000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "gross-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.read.csv('test_5.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "electoral-society",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "| Name |age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "| Krish| 31|        10| 30000|\n",
      "|Sudhan| 30|         8| 25000|\n",
      "|Sudhan| 29|         4| 20000|\n",
      "|  Paul| 24|         3| 20000|\n",
      "|Harsha| 21|         1| 15000|\n",
      "|Sudhan| 23|         2| 18000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "motivated-alarm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name : string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "danish-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name ', 'age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-photography",
   "metadata": {},
   "source": [
    "## Vector assembler make sure that i have all may features together grouped like this \n",
    "\n",
    "[Age, Experience] ------> [new feature]----->independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "mediterranean-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureassembler = VectorAssembler(inputCols=[\"age\",'Experience'], outputCol='Independent features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "hundred-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "output= featureassembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "helpful-purchase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+--------------------+\n",
      "| Name |age|Experience|Salary|Independent features|\n",
      "+------+---+----------+------+--------------------+\n",
      "| Krish| 31|        10| 30000|         [31.0,10.0]|\n",
      "|Sudhan| 30|         8| 25000|          [30.0,8.0]|\n",
      "|Sudhan| 29|         4| 20000|          [29.0,4.0]|\n",
      "|  Paul| 24|         3| 20000|          [24.0,3.0]|\n",
      "|Harsha| 21|         1| 15000|          [21.0,1.0]|\n",
      "|Sudhan| 23|         2| 18000|          [23.0,2.0]|\n",
      "+------+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "driven-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data = output.select(\"Independent Features\", \"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "numerical-consideration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent Features|Salary|\n",
      "+--------------------+------+\n",
      "|         [31.0,10.0]| 30000|\n",
      "|          [30.0,8.0]| 25000|\n",
      "|          [29.0,4.0]| 20000|\n",
      "|          [24.0,3.0]| 20000|\n",
      "|          [21.0,1.0]| 15000|\n",
      "|          [23.0,2.0]| 18000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "attempted-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Train test split\n",
    "train_data, test_data = finalized_data.randomSplit([0.75, 0.25])\n",
    "\n",
    "regressor = LinearRegression(featuresCol='Independent Features', labelCol='Salary')\n",
    "\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "significant-allocation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0, 1589.7436])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Coefficients\n",
    "\n",
    "regressor.coefficients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dominican-japanese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13358.974358973992"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## interceptes:\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "studied-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predictions\n",
    "\n",
    "pred_results= regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "economic-sweet",
   "metadata": {},
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "parental-adolescent",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\calanche\\Desktop\\sample_project_1\\env\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|Independent Features|Salary|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|          [23.0,2.0]| 18000|16538.461538461514|\n",
      "|          [24.0,3.0]| 20000|18128.205128205107|\n",
      "+--------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "coordinated-society",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1666.6666666666897, 2819855.358316973)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError, pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-distance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-taiwan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
